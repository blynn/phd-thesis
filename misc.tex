\chapter{Optimizating Cryptosystems}

Improving the efficiency of a pairing-based cryptosystem can be quite involved:

\begin{enumerate}
\item
Much effort must be devoted to speeding up
integer and finite field arithmetic operations, as they are present
in large amounts at the lowest level. For best results, platform-dependent
hand-coded assembly should be used.
Fortunately a lot of research has been conducted in this area~\cite{taocp2},
and numerous implementations exist, many of them freely available~\cite{gmp, miracl}.
\item
Elliptic curve operations should also be fast. Though a newer area of research,
numerous publications on this subject exist~\cite{bss}.
\item
Speeding up the pairing has only been a high priority relatively recently,
and is the main topic of the next chapter.
\item
Sometimes high-level modifications to a scheme (such as allowing one or
two bits of error, choosing a different group to contain a key, etc.)
will garner savings by
permitting one to
exploit other pairing properties,
move exponentiations or multiplications to cheaper groups,
precompute more elements,
compress or reduce points or pairings, and so on.
\item
Needless to say, each of these classes of optimizations cannot be viewed
in isolation. An optimization that may not usually help could be beneficial
when the behaviour of the whole system is taken into account.
Another possibility is that
one optimization could adversely affect another.
\end{enumerate}

We have concentrated optimizations from all aspects of
a pairing-based cryptosystem into a couple of chapters,
rather than introduce each technique along with the algorithm that it
affects. We prefer presenting shortcuts in one go as a big bag of tricks to
sprinkling them throughout the text.
The last item of the above list relates one reason.

Secondly, an optimization buried in the middle of a section on another topic
may be overlooked or forgotten. This may be less likely when it is found in
a section dedicated to optimizations.

Also, optimizations from different areas can have much in common. For example,
sliding windows and multiexponentiation apply to every repeated-squaring-like
algorithm. Seeing all variants at once can provide a deeper understanding of
a principle, and may even inspire one to apply the same principle to a
different situation.

Lastly, the organization of these chapters reflects good software
engineering practices. When developing a pairing-based cryptography
library from scratch,
the initial goal should be to write finite field arithmetic
routines. The next step is to implement elliptic curve groups.
Once the code for this has been tested satisfactorily, a simple
pairing algorithm should be built.
Only when the pairing is correctly functioning
should one at last consider efficiency improvements.

The slowest parts of the system should be attacked first.
Educated guessing can meet with some success, but
ideally profiling should be employed to identify the
bottlenecks.

\section{Using Existing Libraries}

In the author's pairing implementations, to avoid the drawbacks
of assembly code (e.g. loss of portability, the need to learn platform-specific
low-level tricks, frequent rewrites due to new hardware,
fewer potential developers, increase in debugging difficulty, longer
development time),
pre-existing multiprecision arithmetic libraries
were utilized.

Obviously, the ideal choice is
a library designed for maximum efficiency when using finite
fields sizes typically found in cryptography.
But if no suitable library is available (perhaps due to reasons unrelated
to performance such as lack of portability or licensing issues),
one can easily graft finite field routines on top of
any arbitrary-precision integer library, though doing so can cause losses.

For example, the GMP library has a highly optimized modular exponentiation
routine that uses Montgomery reduction (see below),
but expects the input not to be
in Montgomery form, and also outputs normal integers.

Hence if numbers are already internally stored in Montgomery form,
they must be converted to a normal integer first, and then converted
back to Montgomery form afterwards. Including the unwanted conversions in
GMP itself gives a total of four unnecessary conversions.

Another example is that an external library may be able to handle integers
of arbitrary length, but a particular deployment of a cryptosystem is
concerned only with integers in a certain range. Routines that know
in advance how many bytes their input integers take can be faster than
a more general routine.

\section{All or Nothing}

The nature of cryptography in a finite field requiring $n$ bytes per element
leads to the value of an integer data structure
often containing either zero, or an integer roughly $n$ bytes in length.
This is because many cryptographic operations can be viewed as a series of
arithmetic operations on $n$-byte numbers uniformly chosen at random.

One approach to exploit this is to attach a flag to every integer variable
that signifies when the variable contains the zero value. Then operations
such as addition and multiplication check this flag: if one of the input
variables is zero, the output is trivial to determine and no looping
is required, otherwise the computation should proceed on all $n$ bytes as
with high probability each of the $n$ bytes will be nonzero and thus
contribute to the result.

In some cases it may help to test for one as well.

%Of course, one could rewrite higher level algorithms to avoid these cases
%but this can be tedious and may only be negligibly better.

\section{Montgomery Reduction}

For RSA or a discrete-log system in a finite field, it is recommended
only to use Montgomery reduction during an exponentiation
as the time lost from switching back and forth between representations
does not compensate for the savings gained for individual multiplications~\cite{handbook}.

Pairing-based cryptosystems on the other hand can benefit from having
all coordinates of points stored in Montgomery representation, with conversion
only during input and output (though even then, there is no reason why keys
and such could not be stored in Montgomery representation). This is because
during point additions and multiplications as well as pairing computations,
there are many finite field operations but never a need to convert the
coordinates back to its normal representation.

Division is slower using Montgomery representation, but this drawback is
of no concern if projective coordinates are used (see below),
where each division is replaced by a few multiplications in any case.

The following algorithms can be found in several textbooks in the
field~\cite{bss, handbook}.

Suppose we are implementing the finite field $\F_p$ for some prime $p$.
Let $b$ be the word size of the machine. Let $R = b^t$ be the smallest
power of $b$ greater than $p$. Let $p' = -p^{-1} \pmod b$.

We note since $b$ is a power of 2, one can use a specialized procedure
to find $p'$~\cite[Algorithm II.5]{bss}. However this is usually unimportant
because $p'$ can be precomputed.

Expressions modulo $b$ should obviously be implemented as
operations on machine words. Similarly, multiplications and divisions
involving $R$ and $b$ are simply bit shifts.

Then we represent an element $x\in\F_p$ as $xR$. We only convert back
for input/output operations.

The first algorithm allows us to convert quickly from Montgomery
representation. It applies to any nonnegative integer $y < pR$.
The $i$th most significant machine word of $y$ is
denoted by $y_i$.

\begin{algorithm}
\caption {Montgomery Reduction: $z \gets yR^{-1}$}
\begin{algorithmic}[1]
\FOR {$i=0$ to $t-1$}
\STATE $u\gets y_i p' \pmod b$
\STATE $y\gets y + upb^i$
\ENDFOR
\STATE $z\gets y/R$
\IF {$z\ge p$}
\STATE $z\gets z-p$
\ENDIF
\end{algorithmic}
\end{algorithm}

The next algorithm shows why multiplication is faster. Note
there is only one step requiring the product of a multiprecision integer
and a single word: the other multiplications are single precision.

\begin{algorithm}
\caption {Montgomery Multiplication: $Z = X Y R^{-1} \pmod p$}
\begin{algorithmic}[1]
\STATE $Z\gets 0$
\FOR {$i=0$ to $t-1$}
\STATE $u\gets (z_0 + s_i y_0) p' \pmod b$
\STATE $Z\gets (Z+x_i y + u p)/b$
\ENDFOR
\IF {$Z\ge p$}
\STATE $Z\gets Z-p$
\ENDIF
\end{algorithmic}
\end{algorithm}

Inversion is slower: given $xR$, we use a standard inversion algorithm
to find $x^{-1}R^{-1}$. Then performing a Montgomery multiplication with
$R^3 \pmod p$ (which has been precomputed) yields $x^{-1}R$.

\section{Cube Roots}

When $q = 2 \pmod 3$, for any $x \in \Fq$ we have
$(x^{-(q-2)/3})^3 = x^{1-(q-1)} = x / x^{q-1} = x$,
thus cube roots can be quickly found via exponentiaton
by $-(q-2)/3$. Observe this also means every element of $\Fq$ has
a cube root, which implies cube roots are unique.

Thus with curves of the form $Y^2 = X^3 + b$ over such fields,
a better way of finding random points is to choose $Y$ randomly and
solve for $X$, which involves taking a cube root. Additionally,
point reduction and compression can be achieved by discarding the
$x$-coordinate and only recording the $y$-coordinate. In particular,
for point compression there is no need to store an extra bit and
for point reduction no information is lost.

\section{Random Points}

Let $E : Y^2 = X^3 + a X + b$ be an elliptic curve and suppose we
are working within a cyclic subgroup $G$ of order $r$
of the points of $E$.

To find a random point in $G$, one can randomly choose an $x$-coordinate and
attempt to solve the equation $E$ for the $y$-coordinate, which involves
a finite-field square root algorithm. If no solution exists, more
$x$-coordinates are chosen until a solution for $y$ can be found.

As soon as valid coordinates are obtained,
point multiplication by an appropriate
factor ensures the resulting point $P$ lies in $G$.

In certain curves noted above, there are advantages to choosing $y$ and
solving for $x$ instead.

In either case,
once such a point is found, future random points can be generated by
picking a random $k \in \{0,...,r-1\}$ and returning $kP$. This
can be faster in some curves, though it is an inferior method on others.

To ensure that the point is uniformly chosen from $G$, the point
$P$ must have order $r$, though in most cases (i.e. when $r$ is a large prime
or a product of large primes) this happens with high probability.

When $G$ is not cyclic, but each point has order dividing $r$,
we may of course extend this technique by finding a linearly independent
basis of $G$ and form some linear combination using them,
where the coefficients are chosen randomly.

However, in this case,
many pairing-based cryptosystems still function even if random
points are picked from a cyclic subgroup of $G$ only, and we merely need
one point of $G$ of order $r$.

One could consider a hashing to a group by
hashing to an integer $k$ in $\{0,...,r-1\}$ and returning $kP$
for some fixed point $P$, but for most cryptosystems this cannot be done.
In many applications, security is compromised if the discrete log of
the outputs of a hash function with respect to some fixed base are always
known.

\section{Dedicated Squaring}

A relatively painless way to improve running times
is to implement squaring routines for every field, ring or group.

In the next section we describe squaring tricks
for $\F_{q^2}$ when $q$ is a prime satisfying $q = 2 \pmod 3$
or $q = 3 \pmod 4$.

For other low degree extensions (e.g. 3, 6),
one can write special cases of generalized Karatsuba squaring and
multiplication algorithms \cite{wpkaratsuba}.

\section{ \label{sec:quadext} Quadratic Field Extensions}

For primes $q=3\pmod 4$, a degree two field extension of $\Fq$
can be implemented as $\Fq[i]$ where $i$ is a square root of $-1$.

We have
\[ (a + i b)^2 = (a - b)(a + b) + i (2 a b) \]
and
\[ (a + i b)(c + i d) = (ac - bd) + i[(a + b)(c + d) - ac - bd] \]

As mentioned in the previous chapter, in $\Fq[i]$,
exponentiation by $q$ can be performed by simply negating the imaginary part:
\[ (a+ib)^q = a - ib \]
which in turn implies computing expressions such as $x z - y z^q$ only
costs four multiplications in $\Fq$, where $x,y,z \in \Fq[i]$.

For primes $q=2\pmod 3$,
the polynomial $X^2 + X + 1$ irreducible in $\Fq$, thus its roots
form an optimal normal basis of $\F_{q^2}$
allowing several shortcuts~\cite{xtr}.

In other words, we compute in $\Fq[\alpha]_{\alpha^2+\alpha+1}$
(viewing $\alpha$ as an indeterminate) and represent elements
as tuples $(a,b) \in \Fq \times \Fq$ (which means $a \alpha + b \alpha^2$).
Note $\alpha^3 = 1$ and $x\in\Fq$ may be written as $-x \alpha - x\alpha^2$.

Then
\[ (a \alpha + b \alpha^2)^2 = b(b-2a)\alpha + a(a-2b)\alpha^2 \]
and
\[ (a \alpha + b \alpha^2)(c \alpha + d \alpha^2) =
(bd-ad-bc)\alpha + (ac-ad-bc)\alpha^2 \]

We also have
\[ (a\alpha + b\alpha^2)^q = b\alpha + a\alpha^2 \]
which again implies computing expressions such as $x z - y z^q$ only
costs four multiplications in $\Fq$, where $x,y,z \in \F_{q^2}$.

\section{ \label{sec:rootfinding} Finding a Root of a Polynomial}

When computing parameters for curves using the CM method, we
need to find a root of a Hilbert polynomial modulo a prime.
As we only want a single root, we may use the Cantor-Zassenhaus
method and skip many steps. For example, we do not care about
the multiplicity of the factors or factors of degree higher than 1.
Thus we may do the folliowing to find a root of a
degree $n$ polynomial $f(x)$ in
$\F_q$.

\begin{enumerate}
\item
Compute $g(x) = \gcd(x^q - x, f(x))$.
\item
If $\deg g = 1$ then output the root and stop.
\item
Pick a random $r \in \Fq$. If $r$ is a root then stop.
\item
Compute $s(x) = (x-r)^{(q-1)/2} \bmod g(x)$.
\item
Compute $g'(x) = \gcd(s(x)+1, g(x))$. This is a proper factor of $g$
with probability $1 - 2^{n-1}$. So if $g'(x) \ne 1$ set $g = g'$ and
goto step 2.
\item
Goto step 3.
\end{enumerate}

\section {Projective Coordinates}

Instead of using a pair of numbers to represent a point,
we can use Jacobian projective coordinates: the triplet
$(x,y,z)$ represents the point $(x/z^2, y/z^3)$.

This allows us to replace inversions with several multiplications in a finite
field, which is usually worth the trouble though the savings
vary from implementation to implementation. Note that Montgomery reduction
complements projective coordinates as it speeds up multiplication at the
expense of inversion.

We quote algorithms for point addition and doubling using projective
coordinates from Blake, Seroussi and Smart~\cite{bss}:

\begin{algorithm}
\caption{Projective Point Addition, $(x_3, y_3, z_3) = (x_1, y_1, z_1) +
(x_2, y_2, z_2)$}
\begin{algorithmic}[1]
\STATE $\lambda_1 \gets x_1 z_2^2$
\STATE $\lambda_2 \gets x_2 z_1^2$
\STATE $\lambda_3 \gets \lambda_1 - \lambda_2$
\STATE $\lambda_4 \gets y_1 z_2^3$
\STATE $\lambda_5 \gets y_2 z_1^3$
\STATE $\lambda_6 \gets \lambda_4 - \lambda_5$
\STATE $\lambda_7 \gets \lambda_1 + \lambda_2$
\STATE $\lambda_8 \gets \lambda_4 + \lambda_5$
\STATE $z_3 \gets z_1 z_2 \lambda_3$
\STATE $x_3 \gets \lambda_6^2 - \lambda_7 \lambda_3^2$
\STATE $\lambda_9 \gets \lambda_7 \lambda_3^2 - 2x_3$
\STATE $y_3 \gets (\lambda_9 \lambda_6 - \lambda_8\lambda_3^3)/2$
\end{algorithmic}
\end{algorithm}

Note when one input is projective and the other affine fewer multiplications
are required. This is called \emph{mixed} addition.

\begin{algorithm}
\caption{Projective Point Doubling, $(x_3, y_3, z_3) = 2(x_1, y_1, z_1)$}
\begin{algorithmic}[1]
\STATE $\lambda_1 \gets 3x_1^2 +a z_1^4$
\STATE $z_3 \gets 2 y_1 z_1$
\STATE $\lambda_2 \gets 4x_1 y_1^2$
\STATE $x_3 \gets \lambda_1^2 - 2\lambda_2$
\STATE $\lambda_3 \gets 8y_1^4$
\STATE $y_3 \gets \lambda_1(\lambda_2 - x_3)-\lambda_3$
\end{algorithmic}
\end{algorithm}

We see the computation of $3x^2 + a z^4$ in general requires three
squarings and one multiplication. (We consider multiplication by
a small constant to be negligible.)

For $a=0$ the only one squaring is required.

For small $a$ or fairly small $a$ with low Hamming weight we can ignore
the cost of the multiplication.

For $a=-3$ we may compute $\lambda_1 = 3(x+z^2)(x-z^2)$ which requires
one multiplication and one squaring.

For $a = -3 d^2$ where $d$ is small or fairly small with low Hamming weight,
we may compute $\lambda_1 = 3(x+dz^2)(x-dz^2)$ which has almost the same cost.

In the following chapter, we shall learn how to
transform a given curve. Doing so allows us
to obtain an $a$ that is faster than the general case.

Cohen, Miyaji and Ono~\cite{cmo} investigate other variants of projective
coordinates, where point doubling or multiplication can be even faster.

\section {Point Multiplication}

Point multiplication is the elliptic curve equivalent of modular
exponentiation, thus optimizations that apply to one often apply to the other.

One difference is that group inversion is much cheaper in elliptic curves:
one simply negates the $y$-coordinate, an operation that is essentially free,
whereas in finite fields one must spend time running the extended
Euclidean algorithm.

This allows one
work with addition-subtraction chains instead of addition chains, that is,
use signed sliding
windows~\cite[Chapter 14]{handbook}~\cite[Section IV.2.5]{bss}.

If it is known in advance that a particular point $P$ will feature in
several point multiplications, preprocessing can be employed, namely
multiples of $P$ used often in the exponentiation routine
are computed and stored.

\section{Multiexponentiation}

When computing expressions
of the form $a P + b Q$ (the equivalent of $g^x h^y$ in
finite fields), rather than computing $a P$ and $b Q$ separately
and adding the results, one can use
a multiexponentiation trick, sometimes referred to as vector addition chains~\cite[Chapter 14]{handbook},
to roughly halve the number of point doublings
(if $a$ and $b$ are about the same length).
In its simplest form, $P + Q$ is precomputed and stored,
then a double-and-add algorithm is used to compute the
final result. Instead of describing the algorithm in full, we present
an example:

To compute $12 P + 7 Q$ one precomputes $P + Q$, then:

\begin{enumerate}
\item
$R \gets P$
\item
$R \gets 2R$
\item
$R \gets R + (P + Q)$
\item
$R \gets 2R$
\item
$R \gets R + Q$
\item
$R \gets 2R$
\item
$R \gets R + Q$
\end{enumerate}

Of course vector-chain-addition exponentiation generalizes
to computing any expression of the form
$a_1 P_1 + ... + a_t P_t$. Multiexponentiation can also be combined with
sliding window techniques.
One drawback is the amount of precomputation needed,
which worsens as $t$ increases, and also as the size of the sliding window
is increased in which case one must store $m P + n Q$ for several $m, n$.

Note the Tonelli-Shanks algorithm contains a multiexponentiation
that benefits from this optimization.

\section{Floating-Point Complex Numbers}

Though not part of any cryptosystem, ideally code that finds suitable
curves should also be fast. For type D and G curves, we must
find Hilbert polynomials. This requires high precision
floating point complex arithmetic.

Packages that provide arbitrary precision floating point numbers
may not include routines for complex arithmetic. When implementing
complex numbers using such a library, one should be aware of a few basic
facts.

Multiplication should use the
tricks for quadratic field extensions described earlier.

To minimize underflow, overflow or precision loss~\cite{numericalrecipes},
one should compute the norm as
\[
|a+ib| = \left\{
\begin{array}{ll}
|a|\sqrt{|1+(b/a)^2|} & \mbox{if $|a| \ge |b|$} \\
|b|\sqrt{|1+(a/b)^2|} & \mbox{if $|a| < |b|$}
\end{array}
\right.
\]
and divide using
\[
\frac{a+ib}{c+id} =
\left\{\begin{array}{ll}
\frac{(a+b(d/c))+i(b-a(d/c))}{d+d(d/c)} & \mbox{if $|c|\ge|d|$} \\
\frac{(a(c/d) + b)+i(b(c/d)-a)}{c(c/d)+d} & \mbox{if $|c|<|d|$} \\
\end{array}\right.
\]
